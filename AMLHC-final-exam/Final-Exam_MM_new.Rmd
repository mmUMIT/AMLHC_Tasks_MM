---
title: "Final Exam"
author: "Melvin van der Mark"
date: "27-June-2024"
output: 
  rmdformats::material:
    use_bookdown: TRUE
    highlight: TRUE
    lightbox: TRUE

    
---

# Introduction

Precision medicine is becoming increasinly relevant in the field of oncology. Comprehensive genomic profiling (CGP) allows for the detailed analysis of tumor tissue. New methods such as next-generation sequencing (NGS) can identify various type of alterations such as: mutations, amplifications, rearragements, fusions and more. The identification of these alterations can help with treatment decisions in the field of targeted therapy. These therapies target the specific alterations found.

Question:
Is is possible to predict the possibility of finding an recommendation based on different features in a clinical setting?

# Data Description

The dataset used contains alterations found for different cancer diagnoses, all patients with solid tumor types were included in the dataset. Gene testing for hematological cancer types to exist, but are not covered in this dataset. The patients were recruited between 2016 and 2022. In addition, the dataset includes other variables such as age, gender, year of birth, date of test, date of result, and test panel. 

## Test 

All the patients were tested with the same NGS test using CGP by exploring 323 genes in samples from the tumor. The test were conducted from 2016 - 2022 to get the most relevant results. A number of alterations have related systemtherapies which are then recommended based on not just the gene alteration but also the diagnosis and other factors. 

## Gene panel

ABL1, ABL2, ACVR1B, AKT1, AKT2, AKT3, ALK, AMER1 (FAM123B), APC, AR, ARAF, ARFRP1, ARID1A, ARID1B, ARID2, ASXL1, ATM, ATR, ATRX, AURKA, AURKB, AXIN1, AXL, BAP1, BARD1, BCL2, BCL2L1, BCL2L2, BCL6, BCOR, BCORL1, BCR, BLM, BRAF, BRCA1, BRCA2, BRD4, BRIP1, BTG1, BTK, C11orf30 (EMSY), CARD11, CBFB, CBL, CCND1, CCND2, CCND3, CCNE1, CD274, CD79A, CD79B, CDC73, CDH1, CDK12, CDK4, CDK6, CDK8, CDKN1A, CDKN1B, CDKN2A, CDKN2B, CDKN2C, CEBPA, CHD2, CHD4, CHEK1, CHEK2, CIC, CREBBP, CRKL, CRLF2, CSF1R, CTCF, CTNNA1, CTNNB1, CUL3, CYLD, DAXX, DDR2, DICER1, DNMT3A, DOT1L, EGFR, EP300, EPHA3, EPHA5, EPHA7, EPHB1, ERBB2, ERBB3, ERBB4, ERG, ERRFI1, ESR1, ETV1, ETV4, ETV5, ETV6, EZH2, FAM123B, FAM46C, FANCA, FANCC, FANCD2, FANCE, FANCF, FANCG, FANCL, FAS, FAT1, FBXW7, FGF10, FGF14, FGF19, FGF23, FGF3, FGF4, FGF6, FGFR1, FGFR2, FGFR3, FGFR4, FH, FLCN, FLT1, FLT3, FLT4, FOXL2, FOXP1, FRS2, FUBP1, GABRA6, GATA1, GATA2, GATA3, GATA4, GATA6, GID4 (C17orf39), GLI1, GNA11, GNA13, GNAQ, GNAS, GPR124, GRIN2A, GRM3, GSK3B, H3F3A, HGF, HNF1A, HRAS, HSD3B1, HSP90AA1, IDH1, IDH2, IGF1R, IGF2, IKBKE, IKZF1, IL7R, INHBA, INPP4B, IRF2, IRF4, IRS2, JAK1, JAK2, JAK3, JUN, KAT6A (MYST3), KDM5A, KDM5C, KDM6A, KDR, KEAP1, KEL, KIT, KLHL6, KMT2A (MLL), KMT2C (MLL3), KMT2D (MLL2), KRAS, LMO1, LRP1B, LYN, LZTR1, MAGI2, MAP2K1, MAP2K2, MAP2K4, MAP3K1, MCL1, MDM2, MDM4, MED12, MEF2B, MEN1, MET, MITF, MLH1, MPL, MRE11A, MSH2, MSH6, MTOR, MUTYH, MYB, MYC, MYCL (MYCL1), MYCN, MYD88, NF1, NF2, NFE2L2, NFKBIA, NKX2-1, NOTCH1, NOTCH2, NOTCH3, NPM1, NRAS, NSD1, NTRK1, NTRK2, NTRK3, NUP93, PAK3, PALB2, PARK2, PAX5, PBRM1, PDCD1LG2, PDGFRA, PDGFRB, PDK1, PIK3C2B, PIK3CA, PIK3CB, PIK3CG, PIK3R1, PIK3R2, PLCG2, PMS2, POLD1, POLE, PPP2R1A, PRDM1, PREX2, PRKAR1A, PRKCI, PRKDC, PRSS8, PTCH1, PTEN, PTPN11, QKI, RAC1, RAD50, RAD51, RAF1, RANBP2, RARA, RB1, RBM10, RET, RICTOR, RNF43, ROS1, RPTOR, RUNX1, RUNX1T1, SDHA, SDHB, SDHC, SDHD, SETD2, SF3B1, SLIT2, SMAD2, SMAD3, SMAD4, SMARCA4, SMARCB1, SMO, SNCAIP, SOCS1, SOX10, SOX2, SOX9, SPEN, SPOP, SPTA1, SRC, STAG2, STAT3, STAT4, STK11, SUFU, SYK, TAF1, TBX3, TERC, TERT (promoter only), TET2, TGFBR2, TMPRSS2, TNFAIP3, TNFRSF14, TOP1, TOP2A, TP53, TSC1, TSC2, TSHR, U2AF1, VEGFA, VHL, WISP3, WT1, XPO1, ZBTB2, ZNF217, ZNF703

```{r, data-prep, include=TRUE, echo = FALSE,warning=FALSE,message=FALSE}

require(rmdformats)
require(rmarkdown)
require(dplyr)
require(forcats)
require(knitr)
require(DT)
require(ggplot2)
require(caret)
require(car)
require(gtsummary)
require(ResourceSelection)
require(pROC)
require(e1071)
require(nnet)
require(glmnet)
require(randomForest)
require(gbm)

genes1 <- readxl::read_xlsx("testdata.xlsx")

#create subset of relevant variables
#select only solid tumors, and rename or prep variables for easier analysis
genes <- genes1 %>% 
  filter( diagnosis_dia_cancertype == "solid") %>% 
  select(patientnr = patientNr, gender, yob, 
                           diagnose = diagnosis_dia_disease, cycle = cycleNr_Test, test_date,
                           stage_at_testing = Disease_stage_at_testing, comp_home_y_n,
                          sample_origin = Sample_origin,
                           alts_y_n = ALTS_y_n, recs_y_n = Recs_Original_y_n
         ) %>% 
  mutate(test_date_year = as.numeric(stringr::str_sub(test_date,7,10)), .after = cycle) %>% 
  mutate(
    alts_y_n = ifelse(alts_y_n == "yes",1,0),
    recs_y_n = ifelse(recs_y_n == "yes",1,0),
    yob = as.numeric(yob), 
    sample_origin = ifelse(is.na(sample_origin),"Unknown",sample_origin)) %>% 
  select(-test_date) %>% filter(!is.na(test_date_year))



```

## Dataset

The dataset dimensions are: `r dim(genes)[1]` observations and `r dim(genes)[2]` columns.  <br>

The dataset contains `r length(unique(genes$patientnr))` <br>
- Patients with alterations `r length(unique(subset(genes, alts_y_n == 1)$patientnr))` <br>
- Patients without alterations `r length(unique(subset(genes, alts_y_n == 0)$patientnr))` <br>

The variables part of the dataset are described in more detail in the table below:

| Variable Name   | Description                                                                                          |
|-----------------|------------------------------------------------------------------------------------------------------|
| Patientnr       | unique identifier                                                                                    |
| gender          | gender either "m" male "f" female                                                                    |
| yob             | year of birth, only contains the year in 4 digits                                                    |
| diagnose        | diagnosis, only solid tumor types included                                                           |
| cycle           | number of test, a patient can have multiple tests                                                    |
| test_date_year  | Year the test was performed between 2014 - 2021                                                      |
| stage_at_testing| Disease stage at timepoint of testing: localized, locally advanced or metastasized                   |
| comp_home_y_n   | Was the test done inhouse or by external company yes or no                                           |
| sample_origin   | What kind of sample was used metastasis, lymph node, blood, or primary tumor                         |
| alts_y_n        | Did the test identify any alteration yes or no                                                       |
| recs_y_n        | Did the test provide any therapy recommendations yes or no                                           |

# Data processing

## ICD-10 Groups

Below you find an overview of the data in table format.
```{r, data-pross-header, include=TRUE, echo = FALSE,warning=FALSE,message=FALSE}

#overview of data
datatable(head(genes), options = list(pageLength = 10), caption = "Head of the Genes Dataframe")

#identify rows with missing data
missing_values <- genes %>% filter(if_any(everything(), ~ is.na(.x) & alts_y_n == 1))
#filter out rows with missing values
genes_ <- genes %>% filter(!patientnr %in% missing_values$patientnr)

```

***

Before we can analyze the data we need to get an overview of frequencies and distributions.

```{r, data-pross-diagnosegroups, include=TRUE, echo = FALSE,warning=FALSE,message=FALSE, fig_width=20, fig_height=12}
#show distribution of tests among diagnoses
genes_ %>% 
  mutate(diagnose = fct_rev(fct_infreq(diagnose))) %>% 
  ggplot(aes(x = diagnose)) + 
  geom_bar(fill = "steelblue") + 
  coord_flip() +
  geom_text(stat = 'count', aes(label = ..count..), hjust = -0.2) +
  ylim(0, 220) +
  labs(x = "Diagnoses", y = "Frequency of tests", caption = "Figure 1: Distribution of tests among diagnoses") +
  theme_minimal()

```

***

It would seem that some groups are rather small

```{r, data-pross-diagnosegroups-frequency, include=TRUE, echo = FALSE,warning=FALSE,message=FALSE}

#therefore remove groups with low N

# Calculate the counts and percentages
group_summary <- genes_ %>%
  filter(diagnose != "Other") %>% 
  mutate(fct_infreq(diagnose )) %>% 
  count(diagnose ) %>%
  mutate(percentage = round(n / sum(n) * 100,2 )) %>% 
  arrange(desc(n)) %>% 
  mutate(N = row_number(), .before = diagnose )

# Display the table using kable
kable(group_summary[1:8,], caption = "Top 8 Counts and Percentages per Diagnose")


group_summary_include <- group_summary[1:8,]
genes_ <- genes %>% filter(diagnose %in% group_summary_include$diagnose)


```


```{r, data-pross-baselines, include=TRUE, echo = FALSE,warning=FALSE,message=FALSE}

#therefore remove groups with low N
library(hrbrthemes)

genes_ %>% 
  mutate(recs_y_n = ifelse(recs_y_n == 1,"Recommendations", "No recommendations")) %>%
  ggplot(aes(x=recs_y_n )) +
  geom_bar(fill=c("blue","orange") )

genes_ %>%
  mutate(age_at_testing = test_date_year - yob) %>% 
  mutate(recs_y_n = ifelse(recs_y_n == 1,"Recommendations", "No recommendations")) %>% 
  ggplot( aes(x=age_at_testing, fill=recs_y_n)) +
    geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
    scale_fill_manual(values=c("#69b3a2", "#404080")) +
    theme_ipsum() +
    labs(fill="")

# Calculate the counts and percentages
genes_ %>%
  filter(!is.na(test_date_year) & !is.na(recs_y_n)) %>% 
  mutate(test_date_year = factor(as.character(test_date_year), levels = c(as.character(seq(2014,2021))))) %>% 
  mutate(recs_y_n = ifelse(recs_y_n == 1,"Recommendations", "No recommendations")) %>%
  group_by(test_date_year,recs_y_n) %>% 
  summarise(value = n()) %>% filter(!is.na(test_date_year)) %>% 
  ggplot( aes(x=test_date_year, fill=recs_y_n, y = value)) +
    geom_bar( stat = "identity", position = "dodge") +
    scale_fill_manual(values=c("#69b3a2", "#404080")) +
    theme_ipsum() +
    labs(fill="")

# Display the table using kable
kable(group_summary[1:8,], caption = "Top 8 Counts and Percentages per Diagnose")


group_summary_include <- group_summary[1:8,]
genes_ <- genes %>% filter(diagnose %in% group_summary_include$diagnose)


```



Before training the model, several preprocessing steps were performed: <br>
- Handling missing values by removing rows with missing data, `r nrow(missing_values)` rows. <br>
- Converting all features to factors to facilitate modeling with a decision tree. <br>
- Removing empty and rare classes to ensure the model's robustness, leaving with `r length(unique(genes_$patientnr))` patients and `r nrow(genes_)` tests.



# Analysis 1: logistic regression


## Occurence of an recommendation - logistic regression


### The model
```{r, data-ana-log, include=TRUE, echo = FALSE,warning=FALSE,message=FALSE}

genes_bu <- genes_

#filter small occurences
genes_ <- genes_ %>% filter(cycle %in% c(as.character(seq(1,5))))

# Categorize age into meaningful classes
genes_$age_at_test <- cut(as.numeric(genes_$test_date_year) - as.numeric(genes_$yob), 
                    breaks = c(18, 35, 50, 65, Inf), 
                    labels = c("Young Adult", "Adult", "Middle Aged", "Senior"),
                    right = FALSE)

# Convert age_group to a factor with the lowest group as reference
genes_$age_at_test <- factor(genes_$age_at_test, levels = c("Young Adult", "Adult", "Middle Aged", "Senior"))


#make factor
levels(genes_$diagnose) <- levels(fct_infreq(genes_$diagnose))
genes_$gender <- factor(genes_$gender)
genes_$cycle <- factor(genes_$cycle, levels = c(as.character(seq(1,5))))
genes_$stage_at_testing <- factor(ifelse(is.na(genes_$stage_at_testing),"Unknown",genes_$stage_at_testing), 
                                  levels = c("localized","localadv","meta","Unknown"))
genes_$test_date_year <- factor(as.character(genes_$test_date_year), levels = c(as.character(seq(2014,2021))))
genes_$comp_home_y_n <- factor(genes_$comp_home_y_n, levels = c("no","yes"))
genes_$sample_origin <- factor(genes_$sample_origin, 
                               levels = c("Primary Tumor", "Metastasis", "Regional Lymph Node","Blood","Other","Unknown" ))
genes_$alts_y_n <- factor(as.character(genes_$alts_y_n), levels = c("0","1"))



# Split the data into training and test sets
set.seed(123)  # For reproducibility
trainIndex <- createDataPartition(genes_$recs_y_n, p = 0.7, list = FALSE, times = 1)
trainData <- genes_[trainIndex, ]
testData <- genes_[-trainIndex, ]

# Ensure testData diagnose levels are consistent with trainData
testData$diagnose <- factor(testData$diagnose, levels = levels(trainData$diagnose))
testData$cycle <- factor(testData$cycle, levels = levels(trainData$cycle))

# Fit the logistic regression model
model <- glm(recs_y_n ~ gender + age_at_test + diagnose + cycle + test_date_year + stage_at_testing + comp_home_y_n +
               sample_origin + alts_y_n, data = trainData, family = binomial)

# Summary of the model
model %>% 
  tbl_regression() %>% 
  bold_labels() %>% 
  bold_p(t = 0.1)

```

### Evaluation

```{r, data-ana-log2, include=TRUE, echo = FALSE,warning=FALSE,message=FALSE}

# Predict on test data

predicted_probs <- predict(model, testData, type = "response")
predicted_classes <- ifelse(predicted_probs > 0.5, 1, 0)

# Ensure predicted_classes and testData$recs_y_n are factors with the same levels
testData$recs_y_n <- factor(testData$recs_y_n)  # Ensure testData$recs_y_n is a factor
predicted_classes <- factor(predicted_classes, levels = levels(testData$recs_y_n))

testData$recs_y_n <- factor(testData$recs_y_n, levels = c(0, 1))  # Ensure testData$recs_y_n is a factor with levels 0 and 1
predicted_classes <- factor(predicted_classes, levels = c(0, 1))


# Confusion Matrix
conf_matrix <- confusionMatrix(predicted_classes, testData$recs_y_n)
print(conf_matrix)

# ROC Curve
roc_curve <- roc(as.numeric(testData$alts_y_n), predicted_probs)
plot(roc_curve)
auc(roc_curve)

# Check VIF for multicollinearity
vif(model)

```
Confusion matrix:  <br>
- True Negatives (TN): 229 <br>
- False Positives (FP): 30 <br>
- False Negatives (FN): 9 <br>
- True Positives (TP): 42 <br>


Variance Inflation Factor (VIF)<br>

Generally, a VIF value greater than 10 indicates high multicollinearity, which is not present here as all values are below 10, suggesting that multicollinearity is not a concern for this model.<br>

Strengths:<br>
- High Accuracy (0.8742): The model correctly predicts about 87.42% of the cases.<br>
- High AUC (0.9992): The model has excellent overall performance.<br>
- High Sensitivity (0.8842) and Specificity (0.8235): Good at identifying both true positives and true negatives.<br>
- Positive Predictive Value (0.9622): High precision for predicting the positive class.<br>
- Low Variance Inflation Factor (VIF): No significant multicollinearity issues in the model.<br>

Limitations:<br>
-Moderate Kappa (0.6073): Indicates moderate agreement between predicted and actual values, which can be improved.<br>
-Low Negative Predictive Value (0.5833): Lower precision for predicting the negative class.<br>
-Mcnemar's Test P-Value (0.001362): Indicates a significant difference between the types of errors made by the model, suggesting potential areas for improvement in balancing the errors <br>



# Analysis 2: random forest


## Occurence of an alteration - random forest
```{r, data-ana2-log, include=TRUE, echo = FALSE,warning=FALSE,message=FALSE}


# Check for missing values
#print(sapply(genes_, function(x) sum(is.na(x))))
set.seed(123)  # For reproducibility
trainIndex <- createDataPartition(genes_$recs_y_n, p = 0.7, list = FALSE, times = 1)
trainData <- genes_[trainIndex, ]
testData <- genes_[-trainIndex, ]

# Ensure testData diagnose levels are consistent with trainData
testData$diagnose <- factor(testData$diagnose, levels = levels(trainData$diagnose))
testData$cycle <- factor(testData$cycle, levels = levels(trainData$cycle))


# Remove rows with missing values
trainData <- na.omit(trainData)
testData <- na.omit(testData)

# train a Random Forest model

train_control <- trainControl(method = "boot", number = 25)

# Train the Random Forest model
rf_model <- train(
  recs_y_n ~ gender + age_at_test + diagnose + cycle + test_date_year + stage_at_testing + comp_home_y_n +
               sample_origin + alts_y_n,
  data = trainData,
  method = "rf",
  trControl = train_control,
  tuneLength = 10
)
rf_model

# Get feature importance using varImp
importance <- varImp(rf_model)

# Plot feature importance
plot(importance, top = 18)

```
Optimal Model<br>

mtry = 5: This value was selected as it resulted in the smallest RMSE.<br>
 Performance Metrics for mtry = 5:<br>
    RMSE: 0.297<br>
    Rsquared: 0.406<br>
    MAE: 0.181<br>



```{r, data-ana2-log2, include=TRUE, echo = FALSE,warning=FALSE,message=FALSE}

# Make predictions
rf_predictions <- predict(rf_model, testData)
predicted_classes2 <- ifelse(rf_predictions > 0.5, 1, 0)

# Convert rf_predictions to a factor with the same levels as testData$Status

testData$recs_y_n <- factor(testData$recs_y_n, levels = c(0, 1))  # Ensure testData$recs_y_n is a factor with levels 0 and 1
predicted_classes2 <- factor(predicted_classes2, levels = c(0, 1))

length(predicted_classes2)
length(testData$recs_y_n)

testData <- testData[1:length(predicted_classes2), ]

# Generate the confusion matrix
conf_matrix <- confusionMatrix(predicted_classes2, testData$recs_y_n)
print(conf_matrix)


```


The confusion matrix and associated statistics indicate that the model is performing well overall. <br>
- The accuracy of the model is 90%, meaning it correctly predicts 90% of the cases.  <br>
- The No Information Rate (NIR), which represents the accuracy of a naive model that always predicts the majority class, is 83.55%.  <br>
- The model's accuracy is significantly better than the NIR, with a p-value of 0.000802, indicating that the model's predictions are not due to chance. <br>

The Kappa statistic of 0.6603 indicates substantial agreement between predictions and actual outcomes
Sensitivity is high at 92.28%, accurately for identifying 0 cases
Specificity is lower at 78.43%, "good" identification of 1 cases


# Conclusion and next steps

I have used a logistic regression model and random forest model<br>
- Logistic regression more sensitive for linear relationship, as opposed to random forest<br>
- Random forest is less influence by outlies and missing values <br>
- Logistic regression easier to interpret than random forest <br>


Model 1 - logistic regression: <br>
- Accuracy: 0.871 <br>
- Kappa: 0.5889 <br>
- Sensitivity: 0.888 <br>
- Specificity: 0.784 <br>

Model 2 - random forest: <br> 
- Accuracy: 0.893 <br>
- Kappa: 0.643 <br>
- Sensitivity: 0.915 <br>
- Specificity: 0.784 <br>

Random forest would be slight better at predicting.<br>

Question:<br>
Is is possible to predict the possibility of finding an recommendation based on different features in a clinical setting? <br>

Yes it is possible, unsuprisingly having a company test and alterations proved to be the biggest factors. In addition, certain diagnoses such as skin cancer and colorectal cancer where also important factors. 

Next steps would be to explore: <br>
- Further investigate the dataset, leave out redundant variables, look for bigger dataset, or include other variables <br>
- Explore further machine learning models. <br>
- Explore the different between company and homemade test <br>







